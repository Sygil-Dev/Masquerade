seed_everything: true

trainer:
  accelerator: auto
  strategy: auto
  devices: 1
  precision: bf16-mixed
  fast_dev_run: 1
  max_steps: 1000000

model:
  class_path: masquerade.models.VQAutoEncoder
  init_args:
    input_key: jpg
    monitor: val/rec_loss
    ckpt_path: null
    ema_decay: null
    ignore_keys: []
    learning_rate: 1e-4
    lr_g_factor: 1.0

    loss:
      class_path: masquerade.modules.losses.VQLPIPSWithDiscriminator
      init_args:
        disc_start: 20001
        perceptual_weight: 0.05
        disc_weight: 0.1

    quantizer:
      class_path: masquerade.modules.vqvae.VectorQuantize2
      init_args:
        n_e: 8192
        vq_embed_dim: 256
        beta: 0.25

    encoder:
      class_path: masquerade.modules.vqvae.ConvEncoder
      init_args:
        in_channels: 3
        out_channels: 3
        block_out_channels: [128, 128, 256, 256, 512]
        layers_per_block: 2
        norm_num_groups: 32
        use_conv_shortcut: False
        conv_downsample: False
        double_z: False

    decoder:
      class_path: masquerade.modules.vqvae.ConvDecoder
      init_args:
        in_channels: 3
        out_channels: 3
        # this is reversed in __init__ so it's actually [512, 256, 256, 128, 128]
        # this way the encoder/decoder can share args
        block_out_channels: [128, 128, 256, 256, 512]
        layers_per_block: 2
        norm_num_groups: 32
        use_conv_shortcut: False
        conv_downsample: False # unused, only here to share args with encoder
        double_z: False

    opt_ae:
      class_path: Adam
      init_args:
        lr: 1e-4
        betas: [0.9, 0.99]
        eps: 1e-8
        weight_decay: 1e-4
        fused: true

    opt_disc:
      class_path: Adam
      init_args:
        lr: 1e-4
        betas: [0.9, 0.99]
        eps: 1e-8
        weight_decay: 1e-4
        fused: true

data:
  class_path: masquerade.dataset.HFDatasetModule
  init_args:
    dataset: "neggles/ine"
    resolution: 256
    tokenizer: "google/t5-v1_1-large"
    streaming: false
    num_proc: 4

lr_scheduler:
  T_max: 10000
  eta_min: 0.0
  last_epoch: -1
  verbose: false
